{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas as pd, xgboost, numpy, textblob, string\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel(path, sheet_name):\n",
    "    return pd.read_excel(path, sheet_name = sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_excel('C:\\\\Users\\\\akash\\\\Desktop\\\\TrainingData.xlsx', 'Raw Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_classes(industry_class):\n",
    "    return industry_class.split(';')[0].split(' (')[0]\n",
    "\n",
    "def remove_stopwords(industry_des):\n",
    "    stop = stopwords.words('english')\n",
    "    return ' '.join([word for word in industry_des.split() if word not in (stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Industry Classifications'] = data['Industry Classifications'].apply(clean_classes)\n",
    "data['Business Description'] = data['Business Description'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_data = data.loc[data['Industry Classifications'].isin(['Banks', 'Healthcare', \n",
    "'Biotechnology', 'Energy', 'Consumer Discretionary', 'Information Technology', \n",
    "'Capital Goods', 'Commercial and Professional Services', 'Application Software',\n",
    "'Communications Equipment', 'Asset Management and Custody Banks', \n",
    "'Consumer Staples', 'Chemicals', 'Application Hosting Services',\n",
    "'Aerospace and Defense', 'Electronic Equipment and Instruments', 'Advertising',\n",
    "'Health Care Technology', 'Auto Components', \n",
    "'Data Processing and Outsourced Services'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mod_data['Company Name']\n",
    "del mod_data['Exchange:Ticker']\n",
    "del mod_data['Company Type']\n",
    "del mod_data['Company Status']\n",
    "del mod_data['Geographic Locations']\n",
    "del mod_data['Security Tickers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(\n",
    "                        mod_data['Business Description'], \n",
    "                        mod_data['Industry Classifications'])\n",
    "\n",
    "# label encode the target variables \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(mod_data['Business Description'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(mod_data['Business Description'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(mod_data['Business Description'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(mod_data['Business Description'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "model = KeyedVectors.load_word2vec_format('C:\\\\Users\\\\akash\\\\Downloads\\\\GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(mod_data['Business Description'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=100)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if word in model.vocab:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "mod_data['char_count'] = mod_data['Business Description'].apply(len)\n",
    "mod_data['word_count'] = mod_data['Business Description'].apply(lambda x: len(x.split()))\n",
    "mod_data['word_density'] = mod_data['char_count'] / (mod_data['word_count']+1)\n",
    "mod_data['punctuation_count'] = mod_data['Business Description'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "mod_data['title_word_count'] = mod_data['Business Description'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "mod_data['upper_case_word_count'] = mod_data['Business Description'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "mod_data['noun_count'] = mod_data['Business Description'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "mod_data['verb_count'] = mod_data['Business Description'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "mod_data['adj_count'] = mod_data['Business Description'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "mod_data['adv_count'] = mod_data['Business Description'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "mod_data['pron_count'] = mod_data['Business Description'].apply(lambda x: check_pos_tag(x, 'pron'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train models other than NNs\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.64367816092\n",
      "NB, WordLevel TF-IDF:  0.520114942529\n",
      "NB, N-Gram Vectors:  0.474137931034\n",
      "NB, CharLevel Vectors:  0.405172413793\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.672413793103\n",
      "LR, WordLevel TF-IDF:  0.640804597701\n",
      "LR, N-Gram Vectors:  0.502873563218\n",
      "LR, CharLevel Vectors:  0.632183908046\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print( \"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print( \"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print( \"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, N-Gram Vectors:  0.534482758621\n"
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.508620689655\n",
      "RF, WordLevel TF-IDF:  0.540229885057\n"
     ]
    }
   ],
   "source": [
    "# Random Forests on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print( \"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# Random Forests on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print( \"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.603448275862\n",
      "Xgb, WordLevel TF-IDF:  0.603448275862\n",
      "Xgb, CharLevel Vectors:  0.612068965517\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "print( \"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "print( \"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
    "print( \"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# ANNs\n",
    "import os, numpy as np\n",
    "glove_dir = 'C:\\\\Users\\\\akash\\\\Desktop\\\\glove.6B\\\\'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "max_words = 20000\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 100, 100)          10000000  \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 64)                640064    \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 10,645,524\n",
      "Trainable params: 10,645,524\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ANN \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "maxlen = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 1043 samples\n",
      "Epoch 1/10\n",
      "348/348 [==============================] - 7s 20ms/step - loss: 3.9230 - acc: 0.4368 - val_loss: 0.1167 - val_acc: 0.9770\n",
      "Epoch 2/10\n",
      "348/348 [==============================] - 6s 18ms/step - loss: 1.0711 - acc: 0.8276 - val_loss: 0.0857 - val_acc: 0.9904\n",
      "Epoch 3/10\n",
      "348/348 [==============================] - 6s 17ms/step - loss: 0.6517 - acc: 0.9138 - val_loss: 0.1329 - val_acc: 0.9904\n",
      "Epoch 4/10\n",
      "348/348 [==============================] - 6s 17ms/step - loss: 0.5934 - acc: 0.9282 - val_loss: 0.1430 - val_acc: 0.9856\n",
      "Epoch 5/10\n",
      "348/348 [==============================] - 7s 19ms/step - loss: 0.3980 - acc: 0.9511 - val_loss: 0.1058 - val_acc: 0.9866\n",
      "Epoch 6/10\n",
      "348/348 [==============================] - 6s 18ms/step - loss: 0.4936 - acc: 0.9540 - val_loss: 0.1637 - val_acc: 0.9789\n",
      "Epoch 7/10\n",
      "348/348 [==============================] - 6s 18ms/step - loss: 0.3890 - acc: 0.9655 - val_loss: 0.2224 - val_acc: 0.9779\n",
      "Epoch 8/10\n",
      "348/348 [==============================] - 6s 18ms/step - loss: 0.2898 - acc: 0.9770 - val_loss: 0.0942 - val_acc: 0.9904\n",
      "Epoch 9/10\n",
      "348/348 [==============================] - 7s 19ms/step - loss: 0.6934 - acc: 0.9368 - val_loss: 0.2763 - val_acc: 0.9521\n",
      "Epoch 10/10\n",
      "348/348 [==============================] - 6s 18ms/step - loss: 0.3854 - acc: 0.9655 - val_loss: 0.1221 - val_acc: 0.9837\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n",
    "                            metrics=['accuracy'])\n",
    "\n",
    "# Switching the train and valid sets for training.\n",
    "history = model.fit(valid_seq_x, valid_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=8,\n",
    "                    validation_data=(train_seq_x, train_y))\n",
    "# Not working because of windows.\n",
    "# model.save_weights('ANN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, None, 8)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, None, 64)          18688     \n",
      "_________________________________________________________________\n",
      "lstm_50 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 20)                1300      \n",
      "=================================================================\n",
      "Total params: 213,012\n",
      "Trainable params: 213,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 8))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "# model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 348 samples, validate on 1043 samples\n",
      "Epoch 1/10\n",
      "348/348 [==============================] - 9s 26ms/step - loss: 2.7383 - acc: 0.1638 - val_loss: 2.7372 - val_acc: 0.1716\n",
      "Epoch 2/10\n",
      "348/348 [==============================] - 5s 15ms/step - loss: 2.6368 - acc: 0.1753 - val_loss: 2.6796 - val_acc: 0.1716\n",
      "Epoch 3/10\n",
      "348/348 [==============================] - 5s 14ms/step - loss: 2.4676 - acc: 0.1868 - val_loss: 2.4288 - val_acc: 0.2426\n",
      "Epoch 4/10\n",
      "348/348 [==============================] - 5s 15ms/step - loss: 2.1505 - acc: 0.2701 - val_loss: 2.3007 - val_acc: 0.3116\n",
      "Epoch 5/10\n",
      "348/348 [==============================] - 5s 15ms/step - loss: 1.9746 - acc: 0.3190 - val_loss: 2.3365 - val_acc: 0.2972\n",
      "Epoch 6/10\n",
      "348/348 [==============================] - 5s 15ms/step - loss: 1.8562 - acc: 0.3391 - val_loss: 2.5219 - val_acc: 0.2426\n",
      "Epoch 7/10\n",
      "348/348 [==============================] - 5s 15ms/step - loss: 1.7243 - acc: 0.3879 - val_loss: 2.5843 - val_acc: 0.1898\n",
      "Epoch 8/10\n",
      "348/348 [==============================] - 5s 15ms/step - loss: 1.6900 - acc: 0.3879 - val_loss: 2.3523 - val_acc: 0.2972\n",
      "Epoch 9/10\n",
      "348/348 [==============================] - 5s 15ms/step - loss: 1.5814 - acc: 0.3937 - val_loss: 2.4761 - val_acc: 0.3030\n",
      "Epoch 10/10\n",
      "348/348 [==============================] - 5s 16ms/step - loss: 1.5064 - acc: 0.4425 - val_loss: 2.3616 - val_acc: 0.2963\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "# Switching the train and valid sets for training.\n",
    "history = model.fit(valid_seq_x, valid_y,\n",
    "                    epochs=10,\n",
    "                    batch_size=8,\n",
    "                    validation_data=(train_seq_x, train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
